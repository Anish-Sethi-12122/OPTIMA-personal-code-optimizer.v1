# OPTIMA — System Architecture & Demo Plan

## 1. System Architecture

OPTIMA is designed as a **deterministic-stochastic hybrid system** running 100% on the client edge. It moves heavy inferential work to Web Workers while keeping the main thread free for rich UI interactions.

### Component Pipeline & Data Flow

1. **User Input** (Main Thread)
   - Code is pasted into the `CodeOptimizerTab` workspace.
   - Triggers `analyzeCode()` from `staticAnalyzer.ts`.

2. **Deterministic Static Analysis** (Main Thread)
   - Extremely fast (<5ms), zero-latency parser.
   - Extracts `lineCount`, `functionCount`, `loopDepth`, `nestingDepth`, and detects algorithm keywords.
   - **Chunking Pipeline**: Files >3200 characters are broken down into logical chunks separated by function/class boundaries `^(function |class |def |fn )`.
   - Result: `StaticMeta` object.

3. **Stochastic Optimization / LLM Reasoning** (Worker Thread)
   - Handled via `RunAnywhere` SDK using `WebAssembly` and `Web Workers`.
   - Prompts are generated by `promptBuilder.ts` using the StaticMeta context.
   - **Acceleration**: Detects `WebGPU` vs. `CPU` via capabilities detection, surfaced to the user.
   - Uses **JSON-forced Few-Shot Prompting** on the Liquid LFM2-350M model to eliminate hallucination.
   - The result is streamed back to the Main Thread.

4. **Structured Parser & Logic Layer** (Main Thread)
   - The `parseResult()` function attempts to parse the strict JSON schema.
   - If the model crashes or fails format, the **Fallback Parser** kicks in, synthesizing a result from the StaticMeta.
   - Diff computations are generated iteratively through `codeDiff.ts` to avoid call-stack overflow on huge files.

5. **Visualization & Rendering** (Main Thread)
   - **ExplainPanel**: Visualizes JSON output into Semantic Cards (Complexity, Bottleneck, Tradeoffs, Confidence Score).
   - **DiffViewer**: Renders line-by-line diff with stats.

---

## 2. Performance Fix Plan

### Threading Model
- **Main Thread**: React DOM, CSS Animations, Static Analysis, Strict JSON Parsing, Diff generation (Iterative Myers algorithm to prevent stack overflow).
- **Web Worker Thread**: LlamaCPP model inference, RAM decoding.

### Non-blocking Execution
- UI never freezes.
- `PipelineIndicator` uses async timeouts between rapid stages (`Parsing → Analyzing → Optimizing → Rendering`) to guarantee visual feedback.
- `ModelStatusBar` utilizes `useModelLoader` to initialize the LLM immediately upon App mount (`autoLoad=true`). This entirely eliminates the "Download" blocking phase from the user's workflow.

---

## 3. Production-Ready Prompt Design

```text
You are an expert {language} code optimizer. Analyze and optimize the code below.

EXAMPLE:
CODE:
[snippet]
OUTPUT:
```json
{"detected_algorithm":"...","time_complexity_before":"...","time_complexity_after":"...","bottleneck":"...","optimization_strategy":"...","estimated_improvement":"...","confidence_score":"...","optimized_code":"...","explanation":"...","tradeoffs":"..."}
```

CONTEXT: Language: {language} | Lines: {N} | Functions: {N} | Loop depth: {N} | Detected algorithm: {algo} | Estimated complexity: {O(N)}
FOCUS: {Focus rule e.g., 'Minimize time/space constraints'}

RULES:
- Return ONLY a single JSON code block (no prose before or after)
- Keep functionality identical
- Fill every field; use "N/A" if not applicable
- Explain WHY original fails at scale in 'explanation', and WHY new approach is asymptotically better
- 'estimated_improvement' should be a range or percentage
- 'confidence_score' should be 0-100%
- 'optimized_code' must be escaped (e.g. \n)
- Be concise: explanation max 3 sentences, tradeoffs max 2 sentences

CODE:
...
```

---

## 4. Frontend Design Plan

The frontend employs a **Dark-First Glassmorphism** aesthetic to project a "Premium Startup" feel.

- **Design System Tokens**: Built on CSS variables (`--bg`, `--primary-glow`, `--bg-glass`) for flawless theme toggling.
- **Micro-interactions**: Subtle hover lifts, skeleton loading shimmers, pulsing "active" states on the pipeline.
- **Typography**: Uses `Inter` for UI clarity and `JetBrains Mono` for code distinctiveness.
- **Component Modularity**: Refactored monolithic files into semantic components: `ModelStatusBar`, `PipelineIndicator`, `ExplainPanel`, `DiffViewer`.

---

## 5. Feature Prioritization

### MVP (Must-Have for Demo - Completed)
1. Heavy computation pushed to Web Workers (RunAnywhere integration).
2. JSON Structured Output and parser.
3. Explanation Layer (Cards, Complexity, Confidence Score, Bottleneck).
4. Diff Viewer with lines added/removed metrics.
5. Large file streaming / chunking (removing hard limits).
6. Auto-loading models (Removed manual download button).

### Stretch (If Time Permits)
1. **GitHub Import**: Fetch code straight from a URL.
2. **Local RAG caching**: Save common "bottlenecks" on indexedDB so the AI doesn't resolve identical code snippets over time.

---

## 6. Risks and Limitations

1. **VRAM constraints**: On low-end systems, WebGPU might crash or exhaust shared memory. *Mitigation:* The explicit CPU/WebGPU toggle + Fallback parsing.
2. **LLM Hallucination on Large Chunks**: 350M models might forget JSON structure when given massive files. *Mitigation:* `CHUNK_SIZE` set to `3200` characters logic, iterative chunk combining, and `FallbackResult` synthesizer.
3. **Execution Delay**: On older phones, 400M parameters can take ~5-15s to spit out the first token. *Mitigation:* The `PipelineIndicator` and `Thinking dots` UI actively signal that work is isolated and safe.

---

## 7. Demo Flow (Judges walkthrough)

1. **Initial Load**: User opens the URL. 
   - Observe header: **"Preparing AI model..."** immediately transitions to a progress ring fetching the model in the background. No manual buttons needed.
   - UI is fully intact.
2. **Action**: User pastes a massive 500-line React/Python file with nested loops.
3. **Observation**: Notice the `Code length indicator` parses the file instantly and detects chunks.
4. **Execution**: User hits "Optimize Code".
   - See the `Pipeline Indicator` visually step through: `Parsing` -> `Analyzing` -> `Optimizing`.
   - UI remains perfectly responsive (can toggle dark mode or highlight code).
   - "Streaming..." text implies active edge generation.
5. **The Reveal**: Pipeline hits `Rendering`. The output replaces the streaming state.
   - **Explain Tab**: Beautiful semantic cards show the exact O(N) complexity before/after, identify the bottleneck scale logic, and show a **Confidence Score** and Estimated % Improvement.
   - **Diff Tab**: User clicks Diff tab and sees an aesthetic GitHub-style unified line diff.
6. **Closing**: Switch header toggle from CPU to **WebGPU**, demonstrating hardware interoperability. "This is what a native intelligence layer looks like."
